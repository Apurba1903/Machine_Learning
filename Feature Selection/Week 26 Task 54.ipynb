{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxOvVQMdbqH8"
      },
      "source": [
        "## `Task` Do feature selection as per metods taught is session 54 on SECOM dataset.\n",
        "\n",
        "Dataset Link : https://archive.ics.uci.edu/ml/datasets/SECOM\n",
        "\n",
        "Drive Link : https://docs.google.com/spreadsheets/d/1dFCe1zgokabsiEr6BbWmMJtiMefkrChpJWLiG_0dDkk/edit?usp=share_link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Rcenqn0Yb5HZ"
      },
      "outputs": [],
      "source": [
        "# Write your Code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I33nmJc-cJs1"
      },
      "source": [
        "### `Solution`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIQC_YRgcIux",
        "outputId": "9926d0f1-ddeb-41f4-c9e8-2cacf956fd1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1567 entries, 0 to 1566\n",
            "Columns: 592 entries, Time to Pass/Fail\n",
            "dtypes: float64(590), int64(1), object(1)\n",
            "memory usage: 7.1+ MB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vQtBXo5cBnDsM2fmfHPm6u72KGUS5FjPHNGMxOfYjA9-CAhmnRpwkIw_rOR3sANJIToiUU__6fbBvig/pub?gid=572763137&single=true&output=csv\")\n",
        "\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "RTA3ifpdc2nt",
        "outputId": "55cae460-6ad2-46ac-9984-8f2fcd9ed3c0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>551</th>\n",
              "      <th>552</th>\n",
              "      <th>553</th>\n",
              "      <th>554</th>\n",
              "      <th>555</th>\n",
              "      <th>556</th>\n",
              "      <th>557</th>\n",
              "      <th>558</th>\n",
              "      <th>559</th>\n",
              "      <th>560</th>\n",
              "      <th>561</th>\n",
              "      <th>562</th>\n",
              "      <th>563</th>\n",
              "      <th>564</th>\n",
              "      <th>565</th>\n",
              "      <th>566</th>\n",
              "      <th>567</th>\n",
              "      <th>568</th>\n",
              "      <th>569</th>\n",
              "      <th>570</th>\n",
              "      <th>571</th>\n",
              "      <th>572</th>\n",
              "      <th>573</th>\n",
              "      <th>574</th>\n",
              "      <th>575</th>\n",
              "      <th>576</th>\n",
              "      <th>577</th>\n",
              "      <th>578</th>\n",
              "      <th>579</th>\n",
              "      <th>580</th>\n",
              "      <th>581</th>\n",
              "      <th>582</th>\n",
              "      <th>583</th>\n",
              "      <th>584</th>\n",
              "      <th>585</th>\n",
              "      <th>586</th>\n",
              "      <th>587</th>\n",
              "      <th>588</th>\n",
              "      <th>589</th>\n",
              "      <th>Pass/Fail</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2008-07-19 11:55:00</td>\n",
              "      <td>3030.93</td>\n",
              "      <td>2564.00</td>\n",
              "      <td>2187.7333</td>\n",
              "      <td>1411.1265</td>\n",
              "      <td>1.3602</td>\n",
              "      <td>100.0</td>\n",
              "      <td>97.6133</td>\n",
              "      <td>0.1242</td>\n",
              "      <td>1.5005</td>\n",
              "      <td>0.0162</td>\n",
              "      <td>-0.0034</td>\n",
              "      <td>0.9455</td>\n",
              "      <td>202.4396</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.9558</td>\n",
              "      <td>414.8710</td>\n",
              "      <td>10.0433</td>\n",
              "      <td>0.9680</td>\n",
              "      <td>192.3963</td>\n",
              "      <td>12.5190</td>\n",
              "      <td>1.4026</td>\n",
              "      <td>-5419.00</td>\n",
              "      <td>2916.50</td>\n",
              "      <td>-4043.75</td>\n",
              "      <td>751.00</td>\n",
              "      <td>0.8955</td>\n",
              "      <td>1.7730</td>\n",
              "      <td>3.0490</td>\n",
              "      <td>64.2333</td>\n",
              "      <td>2.0222</td>\n",
              "      <td>0.1632</td>\n",
              "      <td>3.5191</td>\n",
              "      <td>83.3971</td>\n",
              "      <td>9.5126</td>\n",
              "      <td>50.6170</td>\n",
              "      <td>64.2588</td>\n",
              "      <td>49.3830</td>\n",
              "      <td>66.3141</td>\n",
              "      <td>86.9555</td>\n",
              "      <td>...</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.1827</td>\n",
              "      <td>5.7349</td>\n",
              "      <td>0.3363</td>\n",
              "      <td>39.8842</td>\n",
              "      <td>3.2687</td>\n",
              "      <td>1.0297</td>\n",
              "      <td>1.0344</td>\n",
              "      <td>0.4385</td>\n",
              "      <td>0.1039</td>\n",
              "      <td>42.3877</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>533.8500</td>\n",
              "      <td>2.1113</td>\n",
              "      <td>8.95</td>\n",
              "      <td>0.3157</td>\n",
              "      <td>3.0624</td>\n",
              "      <td>0.1026</td>\n",
              "      <td>1.6765</td>\n",
              "      <td>14.9509</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.5005</td>\n",
              "      <td>0.0118</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>2.3630</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2008-07-19 12:32:00</td>\n",
              "      <td>3095.78</td>\n",
              "      <td>2465.14</td>\n",
              "      <td>2230.4222</td>\n",
              "      <td>1463.6606</td>\n",
              "      <td>0.8294</td>\n",
              "      <td>100.0</td>\n",
              "      <td>102.3433</td>\n",
              "      <td>0.1247</td>\n",
              "      <td>1.4966</td>\n",
              "      <td>-0.0005</td>\n",
              "      <td>-0.0148</td>\n",
              "      <td>0.9627</td>\n",
              "      <td>200.5470</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.1548</td>\n",
              "      <td>414.7347</td>\n",
              "      <td>9.2599</td>\n",
              "      <td>0.9701</td>\n",
              "      <td>191.2872</td>\n",
              "      <td>12.4608</td>\n",
              "      <td>1.3825</td>\n",
              "      <td>-5441.50</td>\n",
              "      <td>2604.25</td>\n",
              "      <td>-3498.75</td>\n",
              "      <td>-1640.25</td>\n",
              "      <td>1.2973</td>\n",
              "      <td>2.0143</td>\n",
              "      <td>7.3900</td>\n",
              "      <td>68.4222</td>\n",
              "      <td>2.2667</td>\n",
              "      <td>0.2102</td>\n",
              "      <td>3.4171</td>\n",
              "      <td>84.9052</td>\n",
              "      <td>9.7997</td>\n",
              "      <td>50.6596</td>\n",
              "      <td>64.2828</td>\n",
              "      <td>49.3404</td>\n",
              "      <td>64.9193</td>\n",
              "      <td>87.5241</td>\n",
              "      <td>...</td>\n",
              "      <td>1.33</td>\n",
              "      <td>0.2829</td>\n",
              "      <td>7.1196</td>\n",
              "      <td>0.4989</td>\n",
              "      <td>53.1836</td>\n",
              "      <td>3.9139</td>\n",
              "      <td>1.7819</td>\n",
              "      <td>0.9634</td>\n",
              "      <td>0.1745</td>\n",
              "      <td>0.0375</td>\n",
              "      <td>18.1087</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>535.0164</td>\n",
              "      <td>2.4335</td>\n",
              "      <td>5.92</td>\n",
              "      <td>0.2653</td>\n",
              "      <td>2.0111</td>\n",
              "      <td>0.0772</td>\n",
              "      <td>1.1065</td>\n",
              "      <td>10.9003</td>\n",
              "      <td>0.0096</td>\n",
              "      <td>0.0201</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>208.2045</td>\n",
              "      <td>0.5019</td>\n",
              "      <td>0.0223</td>\n",
              "      <td>0.0055</td>\n",
              "      <td>4.4447</td>\n",
              "      <td>0.0096</td>\n",
              "      <td>0.0201</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>208.2045</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2008-07-19 13:17:00</td>\n",
              "      <td>2932.61</td>\n",
              "      <td>2559.94</td>\n",
              "      <td>2186.4111</td>\n",
              "      <td>1698.0172</td>\n",
              "      <td>1.5102</td>\n",
              "      <td>100.0</td>\n",
              "      <td>95.4878</td>\n",
              "      <td>0.1241</td>\n",
              "      <td>1.4436</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.9615</td>\n",
              "      <td>202.0179</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.5157</td>\n",
              "      <td>416.7075</td>\n",
              "      <td>9.3144</td>\n",
              "      <td>0.9674</td>\n",
              "      <td>192.7035</td>\n",
              "      <td>12.5404</td>\n",
              "      <td>1.4123</td>\n",
              "      <td>-5447.75</td>\n",
              "      <td>2701.75</td>\n",
              "      <td>-4047.00</td>\n",
              "      <td>-1916.50</td>\n",
              "      <td>1.3122</td>\n",
              "      <td>2.0295</td>\n",
              "      <td>7.5788</td>\n",
              "      <td>67.1333</td>\n",
              "      <td>2.3333</td>\n",
              "      <td>0.1734</td>\n",
              "      <td>3.5986</td>\n",
              "      <td>84.7569</td>\n",
              "      <td>8.6590</td>\n",
              "      <td>50.1530</td>\n",
              "      <td>64.1114</td>\n",
              "      <td>49.8470</td>\n",
              "      <td>65.8389</td>\n",
              "      <td>84.7327</td>\n",
              "      <td>...</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.0857</td>\n",
              "      <td>7.1619</td>\n",
              "      <td>0.3752</td>\n",
              "      <td>23.0713</td>\n",
              "      <td>3.9306</td>\n",
              "      <td>1.1386</td>\n",
              "      <td>1.5021</td>\n",
              "      <td>0.3718</td>\n",
              "      <td>0.1233</td>\n",
              "      <td>24.7524</td>\n",
              "      <td>267.064</td>\n",
              "      <td>0.9032</td>\n",
              "      <td>1.10</td>\n",
              "      <td>0.6219</td>\n",
              "      <td>0.4122</td>\n",
              "      <td>0.2562</td>\n",
              "      <td>0.4119</td>\n",
              "      <td>68.8489</td>\n",
              "      <td>535.0245</td>\n",
              "      <td>2.0293</td>\n",
              "      <td>11.21</td>\n",
              "      <td>0.1882</td>\n",
              "      <td>4.0923</td>\n",
              "      <td>0.0640</td>\n",
              "      <td>2.0952</td>\n",
              "      <td>9.2721</td>\n",
              "      <td>0.0584</td>\n",
              "      <td>0.0484</td>\n",
              "      <td>0.0148</td>\n",
              "      <td>82.8602</td>\n",
              "      <td>0.4958</td>\n",
              "      <td>0.0157</td>\n",
              "      <td>0.0039</td>\n",
              "      <td>3.1745</td>\n",
              "      <td>0.0584</td>\n",
              "      <td>0.0484</td>\n",
              "      <td>0.0148</td>\n",
              "      <td>82.8602</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2008-07-19 14:43:00</td>\n",
              "      <td>2988.72</td>\n",
              "      <td>2479.90</td>\n",
              "      <td>2199.0333</td>\n",
              "      <td>909.7926</td>\n",
              "      <td>1.3204</td>\n",
              "      <td>100.0</td>\n",
              "      <td>104.2367</td>\n",
              "      <td>0.1217</td>\n",
              "      <td>1.4882</td>\n",
              "      <td>-0.0124</td>\n",
              "      <td>-0.0033</td>\n",
              "      <td>0.9629</td>\n",
              "      <td>201.8482</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.6052</td>\n",
              "      <td>422.2894</td>\n",
              "      <td>9.6924</td>\n",
              "      <td>0.9687</td>\n",
              "      <td>192.1557</td>\n",
              "      <td>12.4782</td>\n",
              "      <td>1.4011</td>\n",
              "      <td>-5468.25</td>\n",
              "      <td>2648.25</td>\n",
              "      <td>-4515.00</td>\n",
              "      <td>-1657.25</td>\n",
              "      <td>1.3137</td>\n",
              "      <td>2.0038</td>\n",
              "      <td>7.3145</td>\n",
              "      <td>62.9333</td>\n",
              "      <td>2.6444</td>\n",
              "      <td>0.2071</td>\n",
              "      <td>3.3813</td>\n",
              "      <td>84.9105</td>\n",
              "      <td>8.6789</td>\n",
              "      <td>50.5100</td>\n",
              "      <td>64.1125</td>\n",
              "      <td>49.4900</td>\n",
              "      <td>65.1951</td>\n",
              "      <td>86.6867</td>\n",
              "      <td>...</td>\n",
              "      <td>39.33</td>\n",
              "      <td>0.6812</td>\n",
              "      <td>56.9303</td>\n",
              "      <td>17.4781</td>\n",
              "      <td>161.4081</td>\n",
              "      <td>35.3198</td>\n",
              "      <td>54.2917</td>\n",
              "      <td>1.1613</td>\n",
              "      <td>0.7288</td>\n",
              "      <td>0.2710</td>\n",
              "      <td>62.7572</td>\n",
              "      <td>268.228</td>\n",
              "      <td>0.6511</td>\n",
              "      <td>7.32</td>\n",
              "      <td>0.1630</td>\n",
              "      <td>3.5611</td>\n",
              "      <td>0.0670</td>\n",
              "      <td>2.7290</td>\n",
              "      <td>25.0363</td>\n",
              "      <td>530.5682</td>\n",
              "      <td>2.0253</td>\n",
              "      <td>9.33</td>\n",
              "      <td>0.1738</td>\n",
              "      <td>2.8971</td>\n",
              "      <td>0.0525</td>\n",
              "      <td>1.7585</td>\n",
              "      <td>8.5831</td>\n",
              "      <td>0.0202</td>\n",
              "      <td>0.0149</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>73.8432</td>\n",
              "      <td>0.4990</td>\n",
              "      <td>0.0103</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>2.0544</td>\n",
              "      <td>0.0202</td>\n",
              "      <td>0.0149</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>73.8432</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2008-07-19 15:22:00</td>\n",
              "      <td>3032.24</td>\n",
              "      <td>2502.87</td>\n",
              "      <td>2233.3667</td>\n",
              "      <td>1326.5200</td>\n",
              "      <td>1.5334</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.3967</td>\n",
              "      <td>0.1235</td>\n",
              "      <td>1.5031</td>\n",
              "      <td>-0.0031</td>\n",
              "      <td>-0.0072</td>\n",
              "      <td>0.9569</td>\n",
              "      <td>201.9424</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.5661</td>\n",
              "      <td>420.5925</td>\n",
              "      <td>10.3387</td>\n",
              "      <td>0.9735</td>\n",
              "      <td>191.6037</td>\n",
              "      <td>12.4735</td>\n",
              "      <td>1.3888</td>\n",
              "      <td>-5476.25</td>\n",
              "      <td>2635.25</td>\n",
              "      <td>-3987.50</td>\n",
              "      <td>117.00</td>\n",
              "      <td>1.2887</td>\n",
              "      <td>1.9912</td>\n",
              "      <td>7.2748</td>\n",
              "      <td>62.8333</td>\n",
              "      <td>3.1556</td>\n",
              "      <td>0.2696</td>\n",
              "      <td>3.2728</td>\n",
              "      <td>86.3269</td>\n",
              "      <td>8.7677</td>\n",
              "      <td>50.2480</td>\n",
              "      <td>64.1511</td>\n",
              "      <td>49.7520</td>\n",
              "      <td>66.1542</td>\n",
              "      <td>86.1468</td>\n",
              "      <td>...</td>\n",
              "      <td>1.98</td>\n",
              "      <td>0.4287</td>\n",
              "      <td>9.7608</td>\n",
              "      <td>0.8311</td>\n",
              "      <td>70.9706</td>\n",
              "      <td>4.9086</td>\n",
              "      <td>2.5014</td>\n",
              "      <td>0.9778</td>\n",
              "      <td>0.2156</td>\n",
              "      <td>0.0461</td>\n",
              "      <td>22.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>532.0155</td>\n",
              "      <td>2.0275</td>\n",
              "      <td>8.83</td>\n",
              "      <td>0.2224</td>\n",
              "      <td>3.1776</td>\n",
              "      <td>0.0706</td>\n",
              "      <td>1.6597</td>\n",
              "      <td>10.9698</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.4800</td>\n",
              "      <td>0.4766</td>\n",
              "      <td>0.1045</td>\n",
              "      <td>99.3032</td>\n",
              "      <td>0.0202</td>\n",
              "      <td>0.0149</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>73.8432</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 592 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Time        0        1  ...     588       589  Pass/Fail\n",
              "0  2008-07-19 11:55:00  3030.93  2564.00  ...     NaN       NaN         -1\n",
              "1  2008-07-19 12:32:00  3095.78  2465.14  ...  0.0060  208.2045         -1\n",
              "2  2008-07-19 13:17:00  2932.61  2559.94  ...  0.0148   82.8602          1\n",
              "3  2008-07-19 14:43:00  2988.72  2479.90  ...  0.0044   73.8432         -1\n",
              "4  2008-07-19 15:22:00  3032.24  2502.87  ...  0.0044   73.8432         -1\n",
              "\n",
              "[5 rows x 592 columns]"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_4780\\3241618202.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data[column].fillna(random_series, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# Logistic Model Before Feature Selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Dropping Time Column\n",
        "data = data.drop('Time', axis=1)\n",
        "\n",
        "# Filling NaN Values with Random\n",
        "for column in data.columns:\n",
        "    # Print(\"Column-\", column)\n",
        "    # Get the minimum and maximum values of the column\n",
        "    min_value = data[column].min()\n",
        "    max_value = data[column].max()\n",
        "    \n",
        "    # Generate Random numbers within the range\n",
        "    random_values = np.random.uniform(min_value, max_value, size=data[column].isnull().sum())\n",
        "    \n",
        "    # Create a Series with the Random Values\n",
        "    random_series = pd.Series(random_values, index=data[column][data[column].isnull()].index)\n",
        "    \n",
        "    # Fill NaN values with the random Series\n",
        "    data[column].fillna(random_series, inplace=True)\n",
        "    \n",
        "    # Print\n",
        "    # print(data[column].isnull().sum())\n",
        "    \n",
        "# data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1253, 590)\n",
            "(314, 590)\n",
            "Test Accuracy: 0.9044585987261147\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ACER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "# Separate Features and Target\n",
        "X = data.drop('Pass/Fail', axis=1)\n",
        "y = data['Pass/Fail']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "\n",
        "# Initialize and Train Logistic regression Model\n",
        "log_reg = LogisticRegression(max_iter=10000) # Increase max_iter if it does not converge\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make prediction on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Calculate and Print Accuracy Score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To perform filter-based feature selection on the \"UCI SECOM\" dataset, which has 592 columns and a target column called \"Pass/Fail,\" we can utilize the following methods:\n",
        "\n",
        "1. Duplicate Features:\n",
        "   - Identify and remove duplicate columns from the dataset. Columns with identical values provide redundant information and do not contribute to the prediction task.\n",
        "\n",
        "2. Variance Threshold Method:\n",
        "   - Calculate the variance of each feature.\n",
        "   - Remove features with low variance, as they tend to have little or no predictive power.\n",
        "   - Set a threshold value for variance and remove features below that threshold.\n",
        "\n",
        "3. Correlation:\n",
        "   - Compute the correlation matrix of the features.\n",
        "   - Identify highly correlated features and choose one from each highly correlated group.\n",
        "   - High correlation between features indicates redundancy, and removing one from each correlated group helps reduce multicollinearity.\n",
        "\n",
        "4. ANOVA (Analysis of Variance):\n",
        "   - Perform an ANOVA test between each feature and the target variable (\"Pass/Fail\").\n",
        "   - Select features with a significant impact on the target variable.\n",
        "   - Set a significance level (e.g., p-value threshold) for the test to determine the importance of each feature.\n",
        "\n",
        "5. Chi-Squared:\n",
        "   - Apply the Chi-Squared test between each feature and the target variable, considering both features as categorical.\n",
        "   - Select features with a significant association with the target variable.\n",
        "   - Set a significance level (e.g., p-value threshold) to determine the importance of each feature.\n",
        "\n",
        "Implementing these feature selection methods in Python using the \"UCI SECOM\" dataset can be done as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Columns -  592\n",
            "Number of Columns after removing duplicate columns-  487\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2, f_classif\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "data_path = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vQtBXo5cBnDsM2fmfHPm6u72KGUS5FjPHNGMxOfYjA9-CAhmnRpwkIw_rOR3sANJIToiUU__6fbBvig/pub?gid=572763137&single=true&output=csv\"\n",
        "\n",
        "# Load the DataSet\n",
        "DATA = pd.read_csv(data_path)\n",
        "\n",
        "# Remove Duplicate Features\n",
        "# Get the Subset of columns with Duplicate Values\n",
        "duplicated_cols = DATA.columns[DATA.T.duplicated()]\n",
        "\n",
        "# Remove the Duplicated Columns\n",
        "data = DATA.drop(columns=duplicated_cols)\n",
        "\n",
        "# Drop Time Column\n",
        "data.drop('Time', inplace=True, axis=1)\n",
        "\n",
        "# Number of Columns after removing Duplicate Columns\n",
        "print(\"Number of Columns - \", DATA.shape[1])\n",
        "print(\"Number of Columns after removing duplicate columns- \", data.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Columns after Variance Threshold Method-  315\n"
          ]
        }
      ],
      "source": [
        "# Variance Threshold Method\n",
        "selector = VarianceThreshold(threshold=0.01)\n",
        "sel = selector.fit(data)\n",
        "\n",
        "columns = data.columns[sel.get_support()]\n",
        "\n",
        "data_vt = sel.transform(data)\n",
        "\n",
        "data_vt = pd.DataFrame(data_vt, columns=columns)\n",
        "\n",
        "# Number of Columns after Variance Threshold Method\n",
        "print(\"Number of Columns after Variance Threshold Method- \", data_vt.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Columns after Correlation-  162\n"
          ]
        }
      ],
      "source": [
        "# Correlation\n",
        "\n",
        "corr_matrix = data_vt.corr().abs()\n",
        "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool_))\n",
        "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.8)]\n",
        "\n",
        "data_corr = data_vt.drop(to_drop, axis=1)\n",
        "\n",
        "# Number of Columns after Variance Threshold Method\n",
        "print(\"Number of Columns after Correlation- \", data_corr.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1567 entries, 0 to 1566\n",
            "Columns: 162 entries, 0 to Pass/Fail\n",
            "dtypes: float64(162)\n",
            "memory usage: 1.9 MB\n"
          ]
        }
      ],
      "source": [
        "data_corr.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ANOVA approach can not be done here because there are NaN values inside this dataset. But we can still use ANOVA to each column individually, comparing the target variable which is Pass/Fail column in our case against the non-missing values in that specific column.\n",
        "\n",
        "We will drop NaN values of each column before performing the ANOVA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column: 0 - ANOVA p-value: 0.0\n",
            "Column: 1 - ANOVA p-value: 0.0\n",
            "Column: 2 - ANOVA p-value: 0.0\n",
            "Column: 3 - ANOVA p-value: 0.0\n",
            "Column: 4 - ANOVA p-value: 0.0003805229753154665\n",
            "Column: 6 - ANOVA p-value: 0.0\n",
            "Column: 12 - ANOVA p-value: 0.0\n",
            "Column: 14 - ANOVA p-value: 0.0\n",
            "Column: 15 - ANOVA p-value: 0.0\n",
            "Column: 16 - ANOVA p-value: 0.0\n",
            "Column: 18 - ANOVA p-value: 0.0\n",
            "Column: 19 - ANOVA p-value: 0.0\n",
            "Column: 21 - ANOVA p-value: 0.0\n",
            "Column: 22 - ANOVA p-value: 0.0\n",
            "Column: 23 - ANOVA p-value: 0.0\n",
            "Column: 24 - ANOVA p-value: 5.0211194207923185e-05\n",
            "Column: 25 - ANOVA p-value: 0.0\n",
            "Column: 28 - ANOVA p-value: 0.0\n",
            "Column: 29 - ANOVA p-value: 0.0\n",
            "Column: 31 - ANOVA p-value: 0.0\n",
            "Column: 32 - ANOVA p-value: 0.0\n",
            "Column: 33 - ANOVA p-value: 0.0\n",
            "Column: 34 - ANOVA p-value: 0.0\n",
            "Column: 35 - ANOVA p-value: 0.0\n",
            "Column: 37 - ANOVA p-value: 0.0\n",
            "Column: 38 - ANOVA p-value: 0.0\n",
            "Column: 39 - ANOVA p-value: 0.0\n",
            "Column: 40 - ANOVA p-value: 0.0\n",
            "Column: 41 - ANOVA p-value: 0.0\n",
            "Column: 43 - ANOVA p-value: 0.0\n",
            "Column: 44 - ANOVA p-value: 0.0\n",
            "Column: 45 - ANOVA p-value: 0.0\n",
            "Column: 47 - ANOVA p-value: 0.0\n",
            "Column: 48 - ANOVA p-value: 0.0\n",
            "Column: 51 - ANOVA p-value: 0.0\n",
            "Column: 55 - ANOVA p-value: 0.0\n",
            "Column: 59 - ANOVA p-value: 1.2552004335164544e-54\n",
            "Column: 61 - ANOVA p-value: 0.0\n",
            "Column: 62 - ANOVA p-value: 0.0\n",
            "Column: 63 - ANOVA p-value: 0.0\n",
            "Column: 64 - ANOVA p-value: 0.0\n",
            "Column: 67 - ANOVA p-value: 0.023676043778288868\n",
            "Column: 68 - ANOVA p-value: 0.0\n",
            "Column: 71 - ANOVA p-value: 0.0\n",
            "Column: 72 - ANOVA p-value: 0.0\n",
            "Column: 74 - ANOVA p-value: 0.0\n",
            "Column: 83 - ANOVA p-value: 0.0\n",
            "Column: 88 - ANOVA p-value: 0.0\n",
            "Column: 90 - ANOVA p-value: 0.0\n",
            "Column: 96 - ANOVA p-value: 0.0\n",
            "Column: 110 - ANOVA p-value: 0.0\n",
            "Column: 111 - ANOVA p-value: 0.0\n",
            "Column: 115 - ANOVA p-value: 0.0\n",
            "Column: 117 - ANOVA p-value: 0.0\n",
            "Column: 120 - ANOVA p-value: 0.0\n",
            "Column: 122 - ANOVA p-value: 0.0\n",
            "Column: 123 - ANOVA p-value: 0.0\n",
            "Column: 125 - ANOVA p-value: 0.0\n",
            "Column: 126 - ANOVA p-value: 0.0\n",
            "Column: 128 - ANOVA p-value: 0.0\n",
            "Column: 129 - ANOVA p-value: 1.053563031665293e-20\n",
            "Column: 133 - ANOVA p-value: 0.0\n",
            "Column: 134 - ANOVA p-value: 0.0\n",
            "Column: 135 - ANOVA p-value: 0.0\n",
            "Column: 136 - ANOVA p-value: 0.0\n",
            "Column: 137 - ANOVA p-value: 0.0\n",
            "Column: 138 - ANOVA p-value: 0.0\n",
            "Column: 139 - ANOVA p-value: 0.0\n",
            "Column: 142 - ANOVA p-value: 0.0\n",
            "Column: 150 - ANOVA p-value: 0.0\n",
            "Column: 151 - ANOVA p-value: 1.4166449051091726e-76\n",
            "Column: 158 - ANOVA p-value: 0.0\n",
            "Column: 159 - ANOVA p-value: 3.873435589966139e-233\n",
            "Column: 160 - ANOVA p-value: 1.3135685778323096e-263\n",
            "Column: 161 - ANOVA p-value: 5.651686388148162e-260\n",
            "Column: 162 - ANOVA p-value: 8.401617720519068e-164\n",
            "Column: 163 - ANOVA p-value: 0.0\n",
            "Column: 166 - ANOVA p-value: 0.0\n",
            "Column: 167 - ANOVA p-value: 0.0\n",
            "Column: 169 - ANOVA p-value: 0.0\n",
            "Column: 170 - ANOVA p-value: 0.0\n",
            "Column: 175 - ANOVA p-value: 0.0\n",
            "Column: 177 - ANOVA p-value: 0.0\n",
            "Column: 180 - ANOVA p-value: 0.0\n",
            "Column: 181 - ANOVA p-value: 0.0\n",
            "Column: 182 - ANOVA p-value: 0.0\n",
            "Column: 183 - ANOVA p-value: 0.0\n",
            "Column: 184 - ANOVA p-value: 0.0\n",
            "Column: 185 - ANOVA p-value: 0.0\n",
            "Column: 188 - ANOVA p-value: 0.0\n",
            "Column: 195 - ANOVA p-value: 0.0\n",
            "Column: 198 - ANOVA p-value: 0.0\n",
            "Column: 200 - ANOVA p-value: 0.0\n",
            "Column: 201 - ANOVA p-value: 0.0\n",
            "Column: 208 - ANOVA p-value: 0.0\n",
            "Column: 218 - ANOVA p-value: 0.0\n",
            "Column: 223 - ANOVA p-value: 0.0\n",
            "Column: 245 - ANOVA p-value: 5.80826161681738e-107\n",
            "Column: 255 - ANOVA p-value: 0.0\n",
            "Column: 268 - ANOVA p-value: 0.0\n",
            "Column: 269 - ANOVA p-value: 0.0\n",
            "Column: 416 - ANOVA p-value: 0.0\n",
            "Column: 417 - ANOVA p-value: 0.0\n",
            "Column: 418 - ANOVA p-value: 0.0\n",
            "Column: 419 - ANOVA p-value: 9.552745614823105e-257\n",
            "Column: 423 - ANOVA p-value: 0.0\n",
            "Column: 426 - ANOVA p-value: 0.0\n",
            "Column: 429 - ANOVA p-value: 2.8546782689906276e-183\n",
            "Column: 432 - ANOVA p-value: 7.812152995013835e-189\n",
            "Column: 433 - ANOVA p-value: 7.717688534101184e-240\n",
            "Column: 438 - ANOVA p-value: 0.0\n",
            "Column: 439 - ANOVA p-value: 0.0\n",
            "Column: 442 - ANOVA p-value: 0.0\n",
            "Column: 443 - ANOVA p-value: 0.0\n",
            "Column: 444 - ANOVA p-value: 0.0\n",
            "Column: 460 - ANOVA p-value: 0.0\n",
            "Column: 468 - ANOVA p-value: 6.409943849569831e-267\n",
            "Column: 472 - ANOVA p-value: 0.0\n",
            "Column: 474 - ANOVA p-value: 0.0\n",
            "Column: 476 - ANOVA p-value: 0.0\n",
            "Column: 482 - ANOVA p-value: 0.0\n",
            "Column: 483 - ANOVA p-value: 1.92856351133e-312\n",
            "Column: 484 - ANOVA p-value: 2.61571078101574e-284\n",
            "Column: 485 - ANOVA p-value: 9.761371762975728e-244\n",
            "Column: 486 - ANOVA p-value: 2.686034345517541e-303\n",
            "Column: 487 - ANOVA p-value: 3.675565124765342e-238\n",
            "Column: 488 - ANOVA p-value: 0.0\n",
            "Column: 489 - ANOVA p-value: 0.0\n",
            "Column: 491 - ANOVA p-value: 0.0\n",
            "Column: 492 - ANOVA p-value: 0.0\n",
            "Column: 493 - ANOVA p-value: 0.0\n",
            "Column: 494 - ANOVA p-value: 4.251939667888049e-27\n",
            "Column: 496 - ANOVA p-value: 0.0\n",
            "Column: 499 - ANOVA p-value: 1.3626860172168723e-196\n",
            "Column: 500 - ANOVA p-value: 2.2063853113344103e-170\n",
            "Column: 510 - ANOVA p-value: 0.0\n",
            "Column: 511 - ANOVA p-value: 8.521260175759374e-208\n",
            "Column: 519 - ANOVA p-value: 7.913113072865541e-183\n",
            "Column: 520 - ANOVA p-value: 1.0658612203668937e-122\n",
            "Column: 521 - ANOVA p-value: 1.7490564973490379e-06\n",
            "Column: 523 - ANOVA p-value: 3.9933876625832445e-35\n",
            "Column: 525 - ANOVA p-value: 0.0\n",
            "Column: 526 - ANOVA p-value: 0.0\n",
            "Column: 539 - ANOVA p-value: 0.0\n",
            "Column: 546 - ANOVA p-value: 0.0\n",
            "Column: 547 - ANOVA p-value: 0.0\n",
            "Column: 548 - ANOVA p-value: 0.0\n",
            "Column: 549 - ANOVA p-value: 0.0\n",
            "Column: 550 - ANOVA p-value: 0.0\n",
            "Column: 551 - ANOVA p-value: 0.0\n",
            "Column: 559 - ANOVA p-value: 0.0\n",
            "Column: 562 - ANOVA p-value: 0.0\n",
            "Column: 563 - ANOVA p-value: 0.0\n",
            "Column: 564 - ANOVA p-value: 0.0\n",
            "Column: 569 - ANOVA p-value: 0.0\n",
            "Column: 570 - ANOVA p-value: 0.0\n",
            "Column: 571 - ANOVA p-value: 0.0\n",
            "Column: 572 - ANOVA p-value: 4.0824249018704534e-40\n",
            "Column: 573 - ANOVA p-value: 0.0\n",
            "Column: 581 - ANOVA p-value: 2.0775704433733e-310\n",
            "Column: 585 - ANOVA p-value: 2.7656e-319\n",
            "Number of Columns after Correlation-  101\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Significance Value\n",
        "alpha = 0.05\n",
        "\n",
        "# Columns having p_value less than Alpha\n",
        "column_pvalues = []\n",
        "\n",
        "# Iterate over each column\n",
        "for column in data_corr.iloc[:, :-1].columns:\n",
        "    # Extract the non-missing values in the column\n",
        "    column_data = data_corr[column].dropna()\n",
        "    \n",
        "    # Perform ANOVA with the Target Variable\n",
        "    anova_result = f_oneway(column_data, data_corr['Pass/Fail'])\n",
        "    \n",
        "    # Print the ANOVA results or perform further analysis\n",
        "    print(f\"Column: {column} - ANOVA p-value: {anova_result.pvalue}\")\n",
        "    \n",
        "    if anova_result.pvalue <= alpha:\n",
        "        column_pvalues.append((column, anova_result.pvalue))\n",
        "\n",
        "# Selecting best 100 Features - Lower P-Value means better feature\n",
        "# Sort the Column P-Value in Ascending Order\n",
        "column_pvalues.sort(key=lambda x: x[1])\n",
        "\n",
        "# Select the Top 100 Columns with the Lowest P-Values\n",
        "selected_columns = [column for column, _ in column_pvalues[:100]]\n",
        "\n",
        "data_anova = data_corr[selected_columns+['Pass/Fail']]\n",
        "\n",
        "print(\"Number of Columns after Correlation- \", data_anova.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Chi-Squared cant be done because our Data is Numerical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape -  (1567, 101)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_4780\\3246670434.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data_anova[column].fillna(random_series, inplace=True)\n",
            "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_4780\\3246670434.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_anova[column].fillna(random_series, inplace=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1253, 100)\n",
            "(314, 100)\n",
            "Test Accuracy: 0.9203821656050956\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ACER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "# Shape of Data after Feature Selection\n",
        "print(\"Shape - \", data_anova.shape)\n",
        "\n",
        "# Filling NaN values with Random\n",
        "for column in data_anova.columns:\n",
        "    \n",
        "    # Get the minimum and maximum values of the colum\n",
        "    min_value = data_anova[column].min()\n",
        "    max_value = data_anova[column].max()\n",
        "    \n",
        "    # Generate Random numbers within the range\n",
        "    random_values = np.random.uniform(min_value, max_value, size=data_anova[column].isnull().sum())\n",
        "    \n",
        "    # Create a Series with the Random Values\n",
        "    random_series = pd.Series(random_values, index=data_anova[column][data_anova[column].isnull()].index)\n",
        "    \n",
        "    # Fill NaN values with the random Series\n",
        "    data_anova[column].fillna(random_series, inplace=True)\n",
        "    \n",
        "    # Print\n",
        "    # print(data_anova[column].isnull().sum())\n",
        "\n",
        "# Seperate features and target\n",
        "X = data_anova.drop(\"Pass/Fail\", axis=1)\n",
        "y = data_anova[\"Pass/Fail\"]\n",
        "    \n",
        "# Split Data into Training and Testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    \n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "    \n",
        "    \n",
        "# Initialize and Train Logistic regression Model\n",
        "log_reg = LogisticRegression(max_iter=10000) # Increase max_iter if it does not converge\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make prediction on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Calculate and Print Accuracy Score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
